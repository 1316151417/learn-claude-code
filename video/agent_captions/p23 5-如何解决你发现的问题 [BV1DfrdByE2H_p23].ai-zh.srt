1
00:00:00,040 --> 00:00:04,260
一个充满活力的工作流程可能包含多种不同类型的组件

2
00:00:04,260 --> 00:00:08,260
因此，改进不同组件的工具会大不相同

3
00:00:08,260 --> 00:00:10,400
但我想与大家分享一些通用模式

4
00:00:10,400 --> 00:00:15,110
我发现某些组件可能不基于语言模型

5
00:00:15,110 --> 00:00:19,930
这可能像网络搜索引擎或文本检索组件

6
00:00:19,930 --> 00:00:23,340
如果这是你检索增强生成系统的一部分

7
00:00:23,340 --> 00:00:24,960
可能需要代码执行功能

8
00:00:24,960 --> 00:00:27,580
或者一个单独训练的机器学习模型

9
00:00:27,580 --> 00:00:31,200
可能是语音识别或图片中人物检测等功能

10
00:00:31,200 --> 00:00:37,580
这些非语言模型组件可能有可调参数或超参数

11
00:00:37,580 --> 00:00:38,360
对于网络搜索

12
00:00:38,360 --> 00:00:40,400
可以调整结果数量等参数

13
00:00:40,400 --> 00:00:44,060
或设置网络搜索引擎考虑的时间范围

14
00:00:44,060 --> 00:00:46,480
对于文本检索组件

15
00:00:46,480 --> 00:00:51,480
可能改变相似度阈值以确定文本相似性

16
00:00:51,480 --> 00:00:52,740
或调整分块大小

17
00:00:52,740 --> 00:00:55,010
通常检索系统会进行分块处理

18
00:00:55,010 --> 00:00:57,350
将其拆分为更小块以便匹配

19
00:00:57,350 --> 00:01:00,930
这些超参数可能适用于人物检测

20
00:01:00,930 --> 00:01:02,980
可以调整检测阈值

21
00:01:02,980 --> 00:01:04,319
影响其敏感度

22
00:01:04,319 --> 00:01:06,900
以及识别人物的可能性

23
00:01:06,900 --> 00:01:09,600
这会在假阳性和假阴性之间权衡

24
00:01:09,600 --> 00:01:12,400
如果你没有完全理解我刚才提到的超参数细节

25
00:01:12,400 --> 00:01:13,140
不必担心

26
00:01:13,140 --> 00:01:14,480
细节并不重要

27
00:01:14,480 --> 00:01:17,500
但组件参数通常可调

28
00:01:17,500 --> 00:01:20,780
当然，你也可以尝试替换组件

29
00:01:20,780 --> 00:01:23,420
我在自动生成工作流中经常这样做

30
00:01:23,420 --> 00:01:25,930
我会切换不同的检索增强生成引擎

31
00:01:25,930 --> 00:01:28,150
或更换不同的检索服务提供商等

32
00:01:28,150 --> 00:01:34,760
只需测试其他提供商是否因非语言模型组件多样性表现更好

33
00:01:34,760 --> 00:01:39,130
我认为改进技术会更多样化且依赖于

34
00:01:39,130 --> 00:01:43,690
具体组件的功能，对于语言模型组件

35
00:01:43,690 --> 00:01:45,990
这里有一些可考虑的选项

36
00:01:45,990 --> 00:01:48,900
其中一个可能是尝试优化你的

37
00:01:48,900 --> 00:01:51,720
或许添加更明确的指令

38
00:01:51,720 --> 00:01:54,140
如果你了解少样本提示法

39
00:01:54,140 --> 00:02:00,520
指添加输入和期望输出的具体示例

40
00:02:00,520 --> 00:02:01,960
少样本提示

41
00:02:01,960 --> 00:02:05,229
你可以在深度端眼短课程中学习

42
00:02:05,229 --> 00:02:08,989
是一种提供示例以

43
00:02:08,989 --> 00:02:12,460
帮助生成更优输出的技术

44
00:02:12,460 --> 00:02:16,500
或者尝试不同的AI工具

45
00:02:16,500 --> 00:02:17,520
套件或其他工具

46
00:02:17,520 --> 00:02:20,500
尝试多种语言模型其实很容易

47
00:02:20,500 --> 00:02:24,880
然后使用评估选择最佳模型

48
00:02:24,880 --> 00:02:28,660
如果单步操作对单一模型太复杂

49
00:02:28,660 --> 00:02:31,980
可以考虑将任务分解为更小步骤

50
00:02:31,980 --> 00:02:35,820
或拆分为生成步骤和反思步骤

51
00:02:35,820 --> 00:02:36,540
但更一般地说

52
00:02:36,540 --> 00:02:39,430
如果你有非常复杂的指令

53
00:02:39,430 --> 00:02:40,690
全部在一个步骤内

54
00:02:40,690 --> 00:02:44,020
可能单个模型难以完成所有指令

55
00:02:44,020 --> 00:02:48,040
你可以将任务分解为更小的步骤

56
00:02:48,040 --> 00:02:52,440
比如两到三次调用以准确执行最后

57
00:02:52,440 --> 00:02:55,100
当其他方法不起作用时可以尝试

58
00:02:55,100 --> 00:02:58,600
充分考虑微调模型

59
00:02:58,600 --> 00:03:01,620
这通常比其他选项复杂得多

60
00:03:01,620 --> 00:03:05,900
在开发时间投入上也会更昂贵

61
00:03:05,900 --> 00:03:11,570
但如果你有可用于微调模型的数据

62
00:03:11,570 --> 00:03:15,290
这可能比单纯提示更有效

63
00:03:15,290 --> 00:03:19,650
所以我通常在耗尽其他选项后才微调模型

64
00:03:19,650 --> 00:03:22,320
因为微调通常较为复杂

65
00:03:22,320 --> 00:03:24,620
但对于应用而言

66
00:03:24,620 --> 00:03:25,920
尝试过所有其他方法后

67
00:03:25,920 --> 00:03:26,860
如果我仍然处于

68
00:03:26,860 --> 00:03:27,220
你知道的

69
00:03:27,220 --> 00:03:30,340
比如达到90%或95%的性能

70
00:03:30,340 --> 00:03:33,820
我需要挤出最后几个百分点的提升

71
00:03:33,820 --> 00:03:37,940
有时微调自定义模型是很好的技术

72
00:03:37,940 --> 00:03:41,520
我通常只在成熟应用上这样做

73
00:03:41,520 --> 00:03:43,440
因为成本很高

74
00:03:43,440 --> 00:03:45,290
事实证明当你

75
00:03:45,290 --> 00:03:49,740
选择专辑时有一件事对你很有帮助

76
00:03:49,740 --> 00:03:50,700
作为开发者是

77
00:03:50,700 --> 00:03:56,000
如果你对不同大语言模型的智能或能力有良好直觉

78
00:03:56,000 --> 00:03:58,940
你可以尝试大量模型并看哪个效果最好

79
00:03:58,940 --> 00:04:00,860
但随着使用不同模型

80
00:04:00,860 --> 00:04:05,470
我开始形成哪些模型适合何种任务的直觉

81
00:04:05,470 --> 00:04:06,830
并保持这些直觉

82
00:04:06,830 --> 00:04:10,730
在编写有效问题时也会更高效

83
00:04:10,730 --> 00:04:13,490
以及为任务选择合适模型

84
00:04:13,490 --> 00:04:15,990
我想与大家分享一些想法

85
00:04:15,990 --> 00:04:17,870
关于如何磨练直觉

86
00:04:17,870 --> 00:04:21,430
哪些模型适合你的应用

87
00:04:21,430 --> 00:04:26,070
让我们通过使用模型执行指令的例子说明

88
00:04:26,070 --> 00:04:30,800
以删除或屏蔽pii（个人身份信息）

89
00:04:30,800 --> 00:04:33,600
所以删除私密敏感信息

90
00:04:33,600 --> 00:04:38,660
例如使用模型总结客户通话

91
00:04:38,660 --> 00:04:41,880
则可能生成七月十四日的摘要

92
00:04:41,880 --> 00:04:46,060
第二第三条涉及杰西卡·阿尔瓦雷斯的社会安全号码及地址

93
00:04:46,060 --> 00:04:47,680
支持工单等信息

94
00:04:47,680 --> 00:04:50,220
这段文本包含大量敏感

95
00:04:50,220 --> 00:04:53,380
个人身份信息现在

96
00:04:53,380 --> 00:04:57,450
假设我们需要从此类摘要中移除所有pii

97
00:04:57,450 --> 00:05:02,800
因为我们需要用于下游客户分析

98
00:05:02,800 --> 00:05:04,240
并保护客户信息

99
00:05:04,240 --> 00:05:08,510
在下游分析前需先清除pii

100
00:05:08,510 --> 00:05:15,110
因此你可以提示模型识别以下文本中的所有pii

101
00:05:15,110 --> 00:05:19,069
然后返回带有删除标记的文本，包括删除冒号等

102
00:05:20,069 --> 00:05:26,740
事实证明，更大的前沿模型往往更能遵循指令

103
00:05:26,740 --> 00:05:31,459
而较小的模型则擅长回答简单事实性问题

104
00:05:31,459 --> 00:05:34,240
但不太擅长执行指令

105
00:05:34,240 --> 00:05:36,600
如果在较小模型上运行此提示

106
00:05:36,600 --> 00:05:40,260
使用八亿参数的开源llama3.1模型

107
00:05:40,260 --> 00:05:42,120
则可能生成如下输出

108
00:05:42,120 --> 00:05:45,920
它指出识别出的PI是社会保障号和地址

109
00:05:45,920 --> 00:05:48,920
然后按如下方式处理并继续

110
00:05:48,920 --> 00:05:50,220
实际上存在几处错误

111
00:05:50,220 --> 00:05:52,519
未正确遵循指令

112
00:05:52,519 --> 00:05:53,699
显示了列表

113
00:05:53,699 --> 00:05:55,039
然后对文本进行删除处理

114
00:05:55,039 --> 00:05:58,650
然后返回不应生成的另一列表

115
00:05:58,650 --> 00:06:01,630
在PII列表中遗漏了姓名

116
00:06:01,630 --> 00:06:03,880
然后我认为它也没有读取该部分

117
00:06:03,880 --> 00:06:04,560
地址信息

118
00:06:04,560 --> 00:06:05,680
细节并不重要

119
00:06:05,680 --> 00:06:07,940
但未完全遵循这些指令

120
00:06:07,940 --> 00:06:10,530
可能遗漏了部分PII

121
00:06:10,530 --> 00:06:11,150
相比之下

122
00:06:11,150 --> 00:06:12,970
如果使用更智能的模型

123
00:06:12,970 --> 00:06:14,780
一个更擅长执行指令的模型

124
00:06:14,780 --> 00:06:16,900
可能会得到更好的结果如

125
00:06:16,900 --> 00:06:19,420
正确列出了所有PII信息

126
00:06:19,420 --> 00:06:22,160
并正确删除了所有PI

127
00:06:22,160 --> 00:06:28,120
我发现自己不同语言模型提供商专注于不同任务

128
00:06:28,120 --> 00:06:31,070
不同模型确实适合不同任务

129
00:06:31,070 --> 00:06:32,310
有些擅长代码编写

130
00:06:32,310 --> 00:06:33,930
有些更擅长执行指令

131
00:06:33,930 --> 00:06:36,850
有些擅长特定领域的事实

132
00:06:36,850 --> 00:06:41,770
你可以根据直觉判断模型的智能程度

133
00:06:41,770 --> 00:06:44,650
以及指令的可遵循性

134
00:06:44,650 --> 00:06:48,299
从而做出更优模型选择

135
00:06:48,299 --> 00:06:50,799
分享几个实用技巧

136
00:06:50,799 --> 00:06:52,919
建议尝试不同模型

137
00:06:52,919 --> 00:06:55,520
每当有新模型发布时

138
00:06:55,520 --> 00:06:58,960
我通常会测试并尝试不同查询

139
00:06:58,960 --> 00:06:59,380
两者

140
00:06:59,380 --> 00:07:00,140
开源模型

141
00:07:00,140 --> 00:07:02,659
专有模型及开源模型

142
00:07:02,659 --> 00:07:05,779
我发现有时建立个人测试集

143
00:07:05,779 --> 00:07:06,879
可能也很有帮助

144
00:07:06,879 --> 00:07:08,620
准备一组固定问题

145
00:07:08,620 --> 00:07:09,680
测试多个不同模型

146
00:07:09,680 --> 00:07:14,340
有助于评估它们在不同任务中的表现

147
00:07:14,340 --> 00:07:16,280
我经常做的一件事

148
00:07:16,280 --> 00:07:17,960
希望对您有所帮助

149
00:07:17,960 --> 00:07:21,380
我会花大量时间阅读他人提示

150
00:07:21,380 --> 00:07:25,290
有时人们会在互联网上发布他们的提示

151
00:07:25,290 --> 00:07:29,830
经常去阅读它们以了解最佳实践和提示的写法

152
00:07:29,830 --> 00:07:32,510
或者经常与不同公司的朋友交流

153
00:07:32,510 --> 00:07:34,690
包括一些前沿模型公司

154
00:07:34,690 --> 00:07:36,250
并向他们分享我的问题

155
00:07:36,250 --> 00:07:38,100
观察他们是如何提示的

156
00:07:38,100 --> 00:07:42,380
有时我也去查看他人编写的开源包

157
00:07:42,380 --> 00:07:45,280
我非常尊重并下载这些开源包

158
00:07:45,280 --> 00:07:49,640
然后深入研究这些开源包中作者写的提示

159
00:07:49,640 --> 00:07:50,820
为了阅读它们

160
00:07:50,820 --> 00:07:54,570
为了培养自己编写优质提示的直觉

161
00:07:54,570 --> 00:07:57,320
这是我建议你考虑的一种技巧

162
00:07:57,320 --> 00:07:59,740
通过阅读大量他人的提示

163
00:07:59,740 --> 00:08:03,160
这能帮助你提升自己编写提示的能力

164
00:08:03,160 --> 00:08:05,270
我确实经常这样做

165
00:08:05,270 --> 00:08:07,310
我也鼓励你这样做

166
00:08:07,310 --> 00:08:10,610
这能帮你培养关于指令类型的直觉

167
00:08:10,610 --> 00:08:11,870
模型擅长执行哪些指令

168
00:08:11,870 --> 00:08:14,680
何时对不同模型说特定内容

169
00:08:14,680 --> 00:08:18,240
除了与模型互动和阅读他人提示

170
00:08:18,240 --> 00:08:21,560
如果你在工作流中尝试多种不同模型

171
00:08:21,560 --> 00:08:23,940
也能帮助你培养这种直觉

172
00:08:23,940 --> 00:08:27,380
从而了解哪些模型最适合哪些任务

173
00:08:27,380 --> 00:08:31,320
或者通过查看追踪记录获得初步感受

174
00:08:31,320 --> 00:08:34,620
或从组件级或端到端进行分析

175
00:08:34,620 --> 00:08:38,980
这能帮助你评估不同模型在工作流各环节的表现

176
00:08:38,980 --> 00:08:41,220
然后你开始培养关于

177
00:08:41,220 --> 00:08:42,200
不仅仅是性能

178
00:08:42,200 --> 00:08:47,159
可能还包括不同模型的价格与速度权衡

179
00:08:47,159 --> 00:08:50,719
我之所以开发我的工作流

180
00:08:50,719 --> 00:08:55,200
AI套件是因为它能轻松切换和尝试不同模型

181
00:08:55,200 --> 00:08:59,450
这让我在测试和评估时更高效

182
00:08:59,450 --> 00:09:02,190
从而确定哪些模型最适合我的工作流

183
00:09:02,190 --> 00:09:07,460
我们已经讨论了很多如何提升各组件性能

184
00:09:07,460 --> 00:09:11,860
以期提高整个端到端系统的整体性能

185
00:09:11,860 --> 00:09:15,660
除了提升输出质量

186
00:09:15,660 --> 00:09:18,460
在工作流中你可能还需要

187
00:09:18,460 --> 00:09:21,740
同时优化延迟和成本

188
00:09:21,740 --> 00:09:24,200
我发现很多团队在开发初期

189
00:09:24,200 --> 00:09:29,440
通常最担心的是输出质量是否足够高

190
00:09:29,440 --> 00:09:33,530
但当系统运行良好并投入生产后

191
00:09:33,530 --> 00:09:38,950
往往也有价值使其运行更快且成本更低

192
00:09:38,950 --> 00:09:39,990
所以在下一视频

193
00:09:39,990 --> 00:09:46,020
我们将看看如何优化工作负载的成本和延迟

