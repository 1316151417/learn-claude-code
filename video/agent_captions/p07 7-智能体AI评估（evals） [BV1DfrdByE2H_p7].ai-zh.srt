1
00:00:00,120 --> 00:00:03,380
我曾与多个团队合作构建基因工作流

2
00:00:03,380 --> 00:00:06,930
我发现其中一个最大预测因素是

3
00:00:06,930 --> 00:00:09,430
能否真正做好某件事

4
00:00:09,430 --> 00:00:11,870
与效率低下形成对比

5
00:00:11,870 --> 00:00:17,140
是否能够驱动严格的评估流程

6
00:00:17,140 --> 00:00:19,760
因此你推动评估的能力

7
00:00:19,760 --> 00:00:26,070
对基因工作流的构建效果影响巨大

8
00:00:26,070 --> 00:00:26,830
在本视频中

9
00:00:26,830 --> 00:00:29,810
我们将快速概述如何构建评估

10
00:00:29,810 --> 00:00:35,370
这将是课程后续模块深入探讨的主题

11
00:00:35,370 --> 00:00:39,250
因此让我们看看构建一个enworkflow后

12
00:00:39,250 --> 00:00:43,030
比如这个用于处理客户订单查询的工作流

13
00:00:43,030 --> 00:00:45,950
实际上很难预先知道

14
00:00:45,950 --> 00:00:48,370
可能出现哪些问题

15
00:00:48,370 --> 00:00:51,830
因此不如在构建评估时

16
00:00:51,830 --> 00:00:52,950
我建议

17
00:00:52,950 --> 00:00:59,960
只需查看输出结果并手动寻找改进空间

18
00:00:59,960 --> 00:01:00,820
例如

19
00:01:00,820 --> 00:01:02,400
你可能阅读大量输出内容

20
00:01:02,400 --> 00:01:07,689
发现它意外提及竞争对手的情况过多

21
00:01:07,689 --> 00:01:11,880
许多企业不希望员工提及竞争对手

22
00:01:11,880 --> 00:01:13,680
因为这会制造尴尬局面

23
00:01:13,680 --> 00:01:15,560
当你阅读这些输出时

24
00:01:15,560 --> 00:01:17,180
可能会发现中心提到

25
00:01:17,180 --> 00:01:20,000
我很高兴你们的产品远优于竞争对手

26
00:01:20,000 --> 00:01:20,760
竞品代码

27
00:01:20,760 --> 00:01:23,320
或有时说当然应该退款

28
00:01:23,320 --> 00:01:24,340
不同于竞品代码

29
00:01:24,340 --> 00:01:25,500
我们退货流程更便捷

30
00:01:25,500 --> 00:01:27,310
你可能会看到这并感叹

31
00:01:27,310 --> 00:01:30,670
我真的不希望它提及竞争对手

32
00:01:30,670 --> 00:01:32,290
这就是一个典型问题

33
00:01:32,290 --> 00:01:37,150
这类问题很难在构建工作流前预见

34
00:01:37,150 --> 00:01:39,350
最佳实践是先构建系统

35
00:01:39,350 --> 00:01:43,639
然后检查找出不足之处

36
00:01:43,639 --> 00:01:47,499
并找到评估和改进的方法

37
00:01:47,499 --> 00:01:51,190
消除仍不满意的环节

38
00:01:51,190 --> 00:01:58,130
假设企业认为提及竞争对手是错误

39
00:01:58,130 --> 00:01:58,650
那么

40
00:01:58,650 --> 00:02:01,970
在消除这些竞争维度时

41
00:02:01,970 --> 00:02:06,520
一种跟踪进展的方式是添加评估

42
00:02:06,520 --> 00:02:08,900
统计此类错误发生频率

43
00:02:08,900 --> 00:02:12,460
如果你有竞争对手名单如竞品公司

44
00:02:12,460 --> 00:02:13,120
竞对代码

45
00:02:13,120 --> 00:02:14,030
其他公司

46
00:02:14,030 --> 00:02:18,140
则可编写代码在输出中搜索

47
00:02:18,140 --> 00:02:21,640
统计提及竞争对手名称的频率

48
00:02:21,640 --> 00:02:25,980
并计算为整体响应的比例

49
00:02:25,980 --> 00:02:29,550
统计误提竞争对手的频率

50
00:02:29,550 --> 00:02:35,390
关于竞争维度的问题有一个客观指标

51
00:02:35,390 --> 00:02:37,910
这意味着要么提到了竞争对手，要么没有提到

52
00:02:37,910 --> 00:02:40,410
而对于客观标准

53
00:02:40,410 --> 00:02:46,479
你可以编写代码检查该特定错误发生的频率

54
00:02:46,479 --> 00:02:48,179
但因为Elm的输出

55
00:02:48,179 --> 00:02:52,860
自由文本也将成为你评估此输出的标准之一

56
00:02:52,860 --> 00:02:54,740
这可能更具主观性

57
00:02:54,740 --> 00:02:59,180
而编写代码生成黑白评分会更困难

58
00:02:59,180 --> 00:03:00,240
在这种情况下

59
00:03:00,240 --> 00:03:05,740
使用LLM作为裁判是评估输出的常见技术

60
00:03:05,740 --> 00:03:06,420
例如

61
00:03:06,420 --> 00:03:10,600
如果你在构建一个研究代理来处理不同主题的研究

62
00:03:10,600 --> 00:03:14,320
然后可以使用另一个LLM并提示它

63
00:03:14,320 --> 00:03:15,100
也许说

64
00:03:15,100 --> 00:03:18,340
为以下文章分配1到5分的质量评分

65
00:03:18,340 --> 00:03:22,080
其中1分是最差，5分是最好的文章

66
00:03:22,080 --> 00:03:25,120
我用Python表达式表示

67
00:03:25,120 --> 00:03:25,520
你知道的

68
00:03:25,520 --> 00:03:27,620
将生成的文章复制粘贴到这里

69
00:03:27,620 --> 00:03:32,690
因此可以提示LLM阅读文章并为其分配质量评分

70
00:03:32,690 --> 00:03:37,000
然后我让研究代理撰写多份不同研究报告

71
00:03:37,000 --> 00:03:40,480
例如关于黑洞科学的最新进展

72
00:03:40,480 --> 00:03:42,900
或使用机器人采摘水果

73
00:03:42,900 --> 00:03:44,760
然后在这个例子中

74
00:03:44,760 --> 00:03:49,540
裁判LLM可能给黑洞文章打3分

75
00:03:49,540 --> 00:03:52,350
机器人采摘文章得4分

76
00:03:52,350 --> 00:03:55,070
当你不断改进研究代理时

77
00:03:55,070 --> 00:03:57,860
希望这些分数会随时间逐渐提升

78
00:03:57,860 --> 00:04:01,880
顺便说一句，Ohm实际上在这方面并不太擅长

79
00:04:01,880 --> 00:04:03,960
至少在1到5分的评分尺度上

80
00:04:03,960 --> 00:04:04,780
你可以试试看

81
00:04:04,780 --> 00:04:07,980
但我个人很少使用这种技术

82
00:04:07,980 --> 00:04:12,760
但在后续模块中你会学到更好的方法来评估输出

83
00:04:12,760 --> 00:04:16,459
获得比1到5分评分更准确的分数

84
00:04:16,459 --> 00:04:18,289
尽管有些人会这样做

85
00:04:18,289 --> 00:04:19,689
或许作为初步筛选

86
00:04:19,689 --> 00:04:22,249
作为裁判类型的评估

87
00:04:22,249 --> 00:04:27,260
仅作为后续将学习的AI评估方法的预览

88
00:04:27,260 --> 00:04:28,000
在这门课程中

89
00:04:28,000 --> 00:04:32,889
你已经听过我如何编写代码评估客观标准

90
00:04:32,889 --> 00:04:34,689
例如是否提及了竞争对手

91
00:04:34,689 --> 00:04:37,709
或使用LLM作为更主观标准的裁判

92
00:04:37,709 --> 00:04:39,460
例如这篇文章的质量如何

93
00:04:39,460 --> 00:04:42,300
但后续你将学习两种主要评估方法

94
00:04:42,300 --> 00:04:43,320
一种是端到端评估

95
00:04:43,320 --> 00:04:46,919
测量整个代理的输出质量

96
00:04:46,919 --> 00:04:48,999
以及组件级评估

97
00:04:48,999 --> 00:04:53,840
我们可能测量工作流程中单一步骤的输出质量

98
00:04:53,840 --> 00:04:58,490
这些方法对驱动开发过程不同部分非常有用

99
00:04:58,490 --> 00:05:00,770
而我经常做的一件事

100
00:05:00,770 --> 00:05:06,030
就是检查中间输出或称为LLM的追踪记录

101
00:05:06,030 --> 00:05:10,030
为了理解它未能达到我的预期之处

102
00:05:10,030 --> 00:05:12,070
我们称之为错误分析

103
00:05:12,070 --> 00:05:14,430
我们只需通读中间输出结果

104
00:05:14,430 --> 00:05:17,700
每一步都尝试发现改进机会

105
00:05:17,700 --> 00:05:19,540
结果发现能够执行恶行

106
00:05:19,540 --> 00:05:22,180
S和错误分析是一项关键技能

107
00:05:22,180 --> 00:05:26,040
在第四模块中我们将更深入探讨这一点

108
00:05:26,040 --> 00:05:26,900
在这门课程中

109
00:05:26,900 --> 00:05:30,460
我们即将完成第一模块的内容再继续前进

110
00:05:30,460 --> 00:05:31,480
我想与大家分享

111
00:05:31,480 --> 00:05:36,270
我认为构建工作流最重要的设计模式

112
00:05:36,270 --> 00:05:39,070
让我们在下一视频中一探究竟

